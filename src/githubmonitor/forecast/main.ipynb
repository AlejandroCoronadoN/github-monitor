{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast pipeline\n",
    "In this script I will guide you to the execution of the ML forecast pipeline. I will add comments about the pipeline process, model trainning and the prediction process. Each stage of the proces can be run independently but we can review and compare our results for a more interactiv experience.\n",
    "\n",
    "## Preprocess\n",
    "The Preprocess scritpt has it's own jupyter notebook where I made an exploratory analysis of the information. In the script verison I only compile the preprocess functions that allows us to run the ML pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandro.coronado/miniconda3/envs/github-monitor/lib/python3.10/site-packages/tslearn/bases/bases.py:15: UserWarning: h5py not installed, hdf5 features will not be supported.\n",
      "Install h5py to use hdf5 features: http://docs.h5py.org/\n",
      "  warn(h5py_msg)\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "from tqdm import tqdm\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the 'preprocess' directory to the Python path\n",
    "current_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "preprocess_dir = os.path.abspath(os.path.join(current_dir, \"preprocess\"))\n",
    "sys.path.append(preprocess_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/alejandro.coronado/Desktop/Github/github-monitor/src/githubmonitor/forecast/preprocess'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the load_raw_data function directly\n",
    "from preprocess_script import  calculate_sample_size, preprocess_data\n",
    "from complete_series import create_weekly_date_dataframe, expand_time_series, create_seasonal_controls, impute_default\n",
    "from feature_engineering import discard_uncompleted_windows, moving_average_variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete Series env variables\n",
    "TEST = True\n",
    "date_column = \"date\"\n",
    "aggregation_cols = [\"year\", \"month\", \"week\"]\n",
    "# Feature Engineering env variables\n",
    "date_column = \"date\"\n",
    "lag_list = [2, 4, 6, 10]\n",
    "rolling_list = [2, 4, 6]\n",
    "evaluation_window = max(lag_list) + max(rolling_list) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_raw_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m raw_data_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommit_history_raw.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Now you can use load_raw_data in your main script\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m \u001b[43mload_raw_data\u001b[49m(raw_data_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_raw_data' is not defined"
     ]
    }
   ],
   "source": [
    "# Specify the path to your raw data file\n",
    "raw_data_path = os.path.join(current_dir, \"data\", \"raw\", \"commit_history_raw.csv\")\n",
    "\n",
    "# Now you can use load_raw_data in your main script\n",
    "df_raw = load_raw_data(raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sample size\n",
    "total_repositories = len(df_raw.groupby([\"repo_author_single\", \"year\", \"week_number\"])[\"commit_count\"].sum())\n",
    "sample_size = calculate_sample_size(total_repositories)\n",
    "\n",
    "# Preprocess the data\n",
    "preprocess_data(df_raw, sample_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    df = pd.read_csv(\"./data/preprocess/commit_history_subset_test.csv\")\n",
    "else:\n",
    "    df = pd.read_csv(\"./data/preprocess/commit_history_subset.csv\")\n",
    "\n",
    "group_id = [\"repo_name\"]\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "df[date_column] = pd.to_datetime(df[date_column])\n",
    "df_index = df.groupby(group_id).first().reset_index()\n",
    "\n",
    "start_date = df[date_column].min()\n",
    "end_date = df[date_column].max()\n",
    "\n",
    "df_dates_week = create_weekly_date_dataframe(\n",
    "    start_date, end_date, week_start=\"sunday\"\n",
    ")  # Choose between sunday or monday\n",
    "df_expand = expand_time_series(df, date_column, df_index, df_dates_week)\n",
    "df_expand = create_seasonal_controls(df_expand, date_column=\"date\")\n",
    "df_all_preproc = impute_default(df_expand, [\"commit_count\"], 0)\n",
    "df_all_preproc = df_all_preproc[\n",
    "    [\"repo_name\", \"year\", \"commit_count\", \"date\", \"month\", \"week\"]\n",
    "]\n",
    "if TEST:\n",
    "    df_all_preproc.to_csv(\n",
    "        \"./data/preprocess/commit_series_expansion_test.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "else:\n",
    "    df_all_preproc.to_csv(\n",
    "        \"./data/preprocess/commit_series_expansion.csv\",\n",
    "        index=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TEST:\n",
    "    df = pd.read_csv(\"./data/preprocess/commit_series_expansion_test.csv\")\n",
    "else:\n",
    "    df = pd.read_csv(\"./data/preprocess/commit_series_expansion.csv\")\n",
    "\n",
    "df_window_mean, df_window_ewm =moving_average_variables(df, date_column, lag_list, rolling_list)\n",
    "\n",
    "df_out = df.merge(df_window_mean, on=[date_column, \"repo_name\"], how=\"inner\")\n",
    "df_out = df_out.merge(df_window_ewm, on=[date_column, \"repo_name\"], how=\"inner\")\n",
    "df_out = df_out.sort_values([\"repo_name\", date_column], ascending=True)\n",
    "\n",
    "df_out[date_column] = pd.to_datetime(df_out[date_column])\n",
    "\n",
    "if TEST:\n",
    "    file_path = \"./data/preprocess/featureengineering_test.csv\"\n",
    "else:\n",
    "    file_path = \"./data/preprocess/featureengineering.csv\"\n",
    "df_out = discard_uncompleted_windows(df_out, evaluation_window, date_column, \"W\")\n",
    "df_out.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "# Hyperparameter Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the 'preprocess' directory to the Python path\n",
    "current_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "preprocess_dir = os.path.abspath(os.path.join(current_dir, \"process\"))\n",
    "sys.path.append(preprocess_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameter_optimization import preprocess_data, hyperparameter_optimization\n",
    "from iterative_prediction import create_iterative_forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the previous script\n",
    "lag_list = [2, 4, 6, 10]\n",
    "rolling_list = [2, 4, 6]\n",
    "date_column = \"date\"\n",
    "evaluation_window = (\n",
    "    max(lag_list) * 7 + max(rolling_list) * 7\n",
    ")  # minimum data to run t\n",
    "prediction_window = 7 * 12  # days_in_week*number_weeks\n",
    "cut_date = pd.to_datetime(\"2021-12-26\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TEST:\n",
    "    file_path = \"./data/preprocess/featureengineering_test.csv\"\n",
    "else:\n",
    "    file_path = \"./data/preprocess/featureengineering.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df = df[~df[\"commit_count\"].isna()]\n",
    "\n",
    "# Data Preprocessing\n",
    "df = preprocess_data(df, date_column)  # 30 days\n",
    "hyperparameter_optimization(\n",
    "    df= df, \n",
    "    target =\"commit_count\", \n",
    "    prediction_window = prediction_window, \n",
    "    evaluation_window = evaluation_window,\n",
    "    cut_date=cut_date,\n",
    "    run_from_main = True\n",
    ")\n",
    "\n",
    "#! NOTE: No data is saved, we only tarinned the models and save them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = True\n",
    "parallel = False\n",
    "prediction_window = 12  # This time is at a Weekly level\n",
    "prediction_contained = True\n",
    "\n",
    "lag_list = [2, 4, 6, 10]\n",
    "rolling_list = [2, 4, 6]\n",
    "cut_date = pd.to_datetime(\"2021-12-26\")\n",
    "evaluation_window = max(lag_list) + max(rolling_list) + 1\n",
    "# ? Note: that we are going to be placed at t='2021-12-26'  the we need to start the iteration at MAX date to cover predictions until '2021-12-26'. Notice also that you will need at least evaluation_window observations before max date in order to create predictions for forecast_start. The code advance one week at a time to recalculate the predictions using previous predictions.\n",
    "\n",
    "if prediction_contained:\n",
    "    forecast_start = cut_date - timedelta(\n",
    "        days=(prediction_window * 7)\n",
    "    )\n",
    "    min_date = forecast_start - timedelta(days=(evaluation_window * 7))\n",
    "else:\n",
    "    #Start from the last day and starts making iterations over the future\n",
    "    forecast_start = cut_date\n",
    "    min_date = forecast_start - timedelta(days=(evaluation_window * 7))\n",
    "\n",
    "for model in [\"xgboost\", \"randomforest\"]:\n",
    "    start_time = time.time()\n",
    "    if TEST:\n",
    "        file_path = \"./data/preprocess/featureengineering_test.csv\"\n",
    "    else:\n",
    "        file_path = \"./data/preprocess/featureengineering.csv\"\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[~df[\"commit_count\"].isna()]\n",
    "    # Data Preprocessing\n",
    "    df = preprocess_data(df, \"date\")  # 30 days\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_predicted_all = create_forcast(df, forecast_start= forecast_start, parallel=parallel, evaluation_window=evaluation_window, min_date=min_date)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = round(end_time - start_time)\n",
    "\n",
    "    logging.info(f\"Time taken for the operation: {elapsed_time} seconds\")\n",
    "    # SAVE\n",
    "    df_predicted_all.to_csv(f\"./data/process/predictions_{model}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enamble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_column = \"date\"\n",
    "TEST = True\n",
    "prediction_start = pd.to_datetime(\"2022-09-01\")\n",
    "retrain_models = True\n",
    "cut_date = pd.to_datetime(\"2021-12-26\")\n",
    "prediction_contained = True\n",
    "prediction_window = 12\n",
    "lag_list = [2, 4, 6, 10]\n",
    "rolling_list = [2, 4, 6]\n",
    "evaluation_window = max(lag_list) + max(rolling_list) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if prediction_contained:\n",
    "    prediction_start = cut_date - timedelta(days=(prediction_window * 7))\n",
    "    min_date = prediction_start - timedelta(days=(evaluation_window * 7))\n",
    "    prediction_end = cut_date\n",
    "\n",
    "else:\n",
    "    # Start from the last day and starts making iterations over the future\n",
    "    prediction_start = cut_date\n",
    "    min_date = prediction_start - timedelta(days=(evaluation_window * 7))\n",
    "    prediction_end = cut_date + timedelta(\n",
    "        days=(prediction_window * 7)\n",
    "    )  # Prediction of the future\n",
    "\n",
    "if TEST:\n",
    "    all_entries_path = \"../data/preprocess/featureengineering_test.csv\"\n",
    "else:\n",
    "    all_entries_path = \"../data/preprocess/featureengineering.csv\"\n",
    "\n",
    "df_all = pd.read_csv(all_entries_path)\n",
    "df_forecast = prepare_all_models(retrain_models)\n",
    "\n",
    "df_forecast[\"Date\"] = pd.to_datetime(df_forecast[\"Date\"])\n",
    "df_forecast = df_forecast[\n",
    "    df_forecast[\"Commit Forecast\"] != df_forecast[\"Commit Real\"]\n",
    "]\n",
    "\n",
    "df_forecast = (\n",
    "    df_forecast[\n",
    "        [\"Repository\", \"Date\", \"Commit Forecast\", \"Commit Real\", \"model_family\"]\n",
    "    ]\n",
    "    .groupby([\"Date\", \"Repository\", \"model_family\"])\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "df_models = df_forecast.pivot(\n",
    "    index=[\"Repository\", \"Date\"], columns=\"model_family\", values=\"Commit Forecast\"\n",
    ").reset_index()\n",
    "df_forecast_month = create_month_dummy(df_models, retrain_models=retrain_models)\n",
    "test_uniqueness_branch_date(df_models)\n",
    "df_forecast_horizon = create_forecast_horizon(df_models.copy())\n",
    "df_target = df_forecast[[\"Repository\", \"Date\", \"Commit Real\"]].copy()\n",
    "df_target = (\n",
    "    df_target.groupby([\"Repository\", \"Date\"])[\"Commit Real\"].first().reset_index()\n",
    ")\n",
    "\n",
    "predicted_labels = create_cluster(\n",
    "    df=df_all, n=8, retrain_models=retrain_models, target=\"commit_count\"\n",
    ")\n",
    "df_clusters = cluster_indicator(df_all, predicted_labels)\n",
    "\n",
    "df_lr = df_models.merge(df_clusters, on=\"Repository\", how=\"left\")\n",
    "test_uniqueness_branch_date(df_lr)\n",
    "test_uniqueness_branch_date(df_forecast_month)\n",
    "\n",
    "df_model_input = df_lr.merge(\n",
    "    df_forecast_month, on=[\"Repository\", \"Date\"], how=\"inner\"\n",
    ")\n",
    "df_model_input = df_model_input.merge(\n",
    "    df_forecast_horizon, on=[\"Repository\", \"Date\"], how=\"inner\"\n",
    ")\n",
    "df_model_input = df_target.merge(\n",
    "    df_model_input, on=[\"Repository\", \"Date\"], how=\"inner\"\n",
    ")\n",
    "test_uniqueness_branch_date(df_model_input)\n",
    "\n",
    "model, df_results = train_elasticnet_ensemble_model(\n",
    "    df_model_input,\n",
    "    target=\"Commit Real\",\n",
    "    retrain_models=retrain_models,\n",
    "    load_model=True,\n",
    ")\n",
    "\n",
    "df_results[\"model_family\"] = \"elasticnet\"\n",
    "date_prediction_cut = pd.to_datetime(\"2023-08-01\")\n",
    "df_forecast = pd.concat(\n",
    "    [df_forecast, df_results], ignore_index=True\n",
    ")  # Check columns\n",
    "\n",
    "df_forecast.to_csv(\"final_predictions.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
