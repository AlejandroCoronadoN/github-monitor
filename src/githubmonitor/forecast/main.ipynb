{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast pipeline\n",
    "In this script I will guide you to the execution of the ML forecast pipeline. I will add comments about the pipeline process, model trainning and the prediction process. Each stage of the proces can be run independently but we can review and compare our results for a more interactiv experience.\n",
    "\n",
    "## Data Selection\n",
    "This notebook takes only a representative subsample of the entire Github data available to train the models. Given the resource and time constraints, we are only using 10% of the total information (2% when TEST = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "from utils.utils import (\n",
    "    interact_categorical_numerical,\n",
    "    perform_standardization,\n",
    "    prepare_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "from tqdm import tqdm\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Append the 'preprocess' directory to the Python path\n",
    "current_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "preprocess_dir = os.path.abspath(os.path.join(current_dir, \"preprocess\"))\n",
    "sys.path.append(preprocess_dir)\n",
    "\n",
    "# Append the 'preprocess' directory to the Python path\n",
    "current_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "process_dir = os.path.abspath(os.path.join(current_dir, \"process\"))\n",
    "sys.path.append(process_dir)\n",
    "\n",
    "TEST = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST option\n",
    "If you only want to see how the data process is being exeucted I recommend setting TEST = True. The pipeline will only run with a subsample of the information and will execute every function much faster. If you are running the projet directly from the repository you will only have acces to this subsample of data, so theis notebook will only run with TEST =True.\n",
    "Read the preprocess.ipynb notebook to learn how the subset was generated and how to pass all the available information to create the production version models of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import the load_raw_data function directly\n",
    "from preprocess_script import  calculate_sample_size, preprocess_data\n",
    "from complete_series import create_weekly_date_dataframe, expand_time_series, create_seasonal_controls, impute_default\n",
    "from feature_engineering import discard_uncompleted_windows, moving_average_variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "#Complete Series env variables\n",
    "date_column = \"date\"\n",
    "aggregation_cols = [\"year\", \"month\", \"week\"]\n",
    "# Feature Engineering env variables\n",
    "date_column = \"date\"\n",
    "lag_list = [2, 4, 6, 10]\n",
    "rolling_list = [2, 4, 6]\n",
    "evaluation_window = max(lag_list) + max(rolling_list) + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "### Preprocess Data\n",
    "The Preprocess scritpt has it's own jupyter notebook where I made an exploratory analysis of the information. In the script verison I only compile the preprocess functions that allows us to run the ML pipeline. in this proproces a subsample of data is crated and the observations are filtered by detecting outliers in commits and Dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Specify the path to your raw data file\n",
    "raw_data_path = os.path.join(current_dir, \"data\", \"raw\", \"commit_history_raw.csv\")\n",
    "\n",
    "\n",
    "# Read the CSV file\n",
    "df_raw = pd.read_csv(raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the sample size\n",
    "total_repositories = len(df_raw.groupby([\"repo_author_single\", \"year\", \"week_number\"])[\"commit_count\"].sum())\n",
    "sample_size = calculate_sample_size(total_repositories)\n",
    "\n",
    "# Preprocess the data\n",
    "df_bounded =preprocess_data(df_raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path= os.path.join(current_dir, \"data/preprocess/commit_history_subset.csv\")\n",
    "df_bounded.to_csv(output_path, index=False)\n",
    "\n",
    "testing_sample = df_bounded.repo_name.unique()[:150]\n",
    "df_testing = df_bounded[df_bounded.repo_name.isin(testing_sample)]\n",
    "logging.info(\"\\n-Dates:\")\n",
    "logging.info(\"Max commits per day: \", df_testing.commit_count.max())\n",
    "logging.info(\"Max date: \", df_testing.date.max())\n",
    "logging.info(\"Min date: \", df_testing.date.min())\n",
    "logging.info(\"Shape: \", df_testing.shape)\n",
    "\n",
    "testing_path= os.path.join(current_dir, \"data/preprocess/commit_history_subset_test.csv\")\n",
    "df_testing.to_csv(testing_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Complete Time Series\n",
    "Raw data is not a valid Time Series format, we only report the ammount of commits reported at each week. However we require that every repository report at least 0 commits for all the weeks in the evaluation period. In other words, we need to input 0 to all the non reported weeks in the GH BigQuery database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    df = pd.read_csv(\"./data/preprocess/commit_history_subset_test.csv\")\n",
    "else:\n",
    "    df = pd.read_csv(\"./data/preprocess/commit_history_subset.csv\")\n",
    "\n",
    "group_id = [\"repo_name\"]\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "df[date_column] = pd.to_datetime(df[date_column])\n",
    "df_index = df.groupby(group_id).first().reset_index()\n",
    "\n",
    "start_date = df[date_column].min()\n",
    "end_date = df[date_column].max()\n",
    "\n",
    "df_dates_week = create_weekly_date_dataframe(\n",
    "    start_date, end_date, week_start=\"sunday\"\n",
    ")  # Choose between sunday or monday\n",
    "df_expand = expand_time_series(df, date_column, df_index, df_dates_week)\n",
    "df_expand = create_seasonal_controls(df_expand, date_column=\"date\")\n",
    "df_all_preproc = impute_default(df_expand, [\"commit_count\"], 0)\n",
    "df_all_preproc = df_all_preproc[\n",
    "    [\"repo_name\", \"year\", \"commit_count\", \"date\", \"month\", \"week\"]\n",
    "]\n",
    "if TEST:\n",
    "    df_all_preproc.to_csv(\n",
    "        \"./data/preprocess/commit_series_expansion_test.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "else:\n",
    "    df_all_preproc.to_csv(\n",
    "        \"./data/preprocess/commit_series_expansion.csv\",\n",
    "        index=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Enginnering\n",
    "As discussed in this script, this project only created lag dependant variables and moving averages. By providing a list of lags and evlaution windows this block of code create a new variable for each combination of the afromentioned list elements. These are the variables with most predictive power in TimeSeries and we can add control variables at the end of the ensemble models (when trainnig the elasticNet model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if TEST:\n",
    "    df = pd.read_csv(\"./data/preprocess/commit_series_expansion_test.csv\")\n",
    "else:\n",
    "    df = pd.read_csv(\"./data/preprocess/commit_series_expansion.csv\")\n",
    "\n",
    "df_window_mean, df_window_ewm =moving_average_variables(df, date_column, lag_list, rolling_list)\n",
    "\n",
    "df_out = df.merge(df_window_mean, on=[date_column, \"repo_name\"], how=\"inner\")\n",
    "df_out = df_out.merge(df_window_ewm, on=[date_column, \"repo_name\"], how=\"inner\")\n",
    "df_out = df_out.sort_values([\"repo_name\", date_column], ascending=True)\n",
    "\n",
    "df_out[date_column] = pd.to_datetime(df_out[date_column])\n",
    "\n",
    "if TEST:\n",
    "    file_path = \"./data/preprocess/featureengineering_test.csv\"\n",
    "else:\n",
    "    file_path = \"./data/preprocess/featureengineering.csv\"\n",
    "df_out = discard_uncompleted_windows(df_out, evaluation_window, date_column, \"W\")\n",
    "df_out.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "## Process\n",
    "### Hyperparameter Optimization\n",
    "Now that the model is preprocessed we can use the new features \n",
    "and additional control variables to train three different models. Each model is trainned with the same features and os \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "from hyperparameter_optimization import  hyperparameter_optimization\n",
    "from iterative_prediction import create_iterative_forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "# From the previous script\n",
    "lag_list = [2, 4, 6, 10]\n",
    "rolling_list = [2, 4, 6]\n",
    "date_column = \"date\"\n",
    "evaluation_window = (\n",
    "    max(lag_list) * 7 + max(rolling_list) * 7\n",
    ")  # minimum data to run t\n",
    "prediction_window = 7 * 12  # days_in_week*number_weeks\n",
    "cut_date = pd.to_datetime(\"2021-12-26\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "if TEST:\n",
    "    file_path = \"./data/preprocess/featureengineering_test.csv\"\n",
    "else:\n",
    "    file_path = \"./data/preprocess/featureengineering.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df = df[~df[\"commit_count\"].isna()]\n",
    "\n",
    "\n",
    "# Data Preprocessing\n",
    "df = prepare_data(df, date_column)  # 30 days\n",
    "os.chdir(current_dir)\n",
    "os.chdir(\"./process\")\n",
    "hyperparameter_optimization(\n",
    "    df= df, \n",
    "    target =\"commit_count\", \n",
    "prediction_window = prediction_window, \n",
    "    evaluation_window = evaluation_window,\n",
    "    cut_date=cut_date,\n",
    "    date_column=\"date\",\n",
    "    xgboost=False,\n",
    "    lgbm=False,\n",
    "    randomforest=True,\n",
    ")\n",
    "os.chdir(\"..\")\n",
    "\n",
    "\n",
    "#! NOTE: No data is saved, we only tarinned the models and save them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative prediction\n",
    "After the models have been trainned, we repeat the prediction process but this time using the best hyperparameters for each model. These prediction can be used to study and analyse each model separaely (read the Regression Anlaysis.ipynb file to look at the results.). Furthermore, each model prediction will become a new feature for the head model of the ensemble model, meaning that the elasticnet model will weight all the results and createa  metaprediction with the feedback of previously trainned models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iterative_prediction import create_iterative_forecast\n",
    "retrain_models = True\n",
    "model_features = [\n",
    "    \"sum_repo_name_on_commit_count_with_roll2_lag2_ewm\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll4_lag2_ewm\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll6_lag2_ewm\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll2_lag4_ewm\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll4_lag4_ewm\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll6_lag4_ewm\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll2_lag6_ewm\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll4_lag6_ewm\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll6_lag6_ewm\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll2_lag10_ewm\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll4_lag10_ewm\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll6_lag10_ewm\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll2_lag2_rolling\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll4_lag2_rolling\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll6_lag2_rolling\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll2_lag4_rolling\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll4_lag4_rolling\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll6_lag4_rolling\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll2_lag6_rolling\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll4_lag6_rolling\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll6_lag6_rolling\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll2_lag10_rolling\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll4_lag10_rolling\",\n",
    "    \"sum_repo_name_on_commit_count_with_roll6_lag10_rolling\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel = False\n",
    "prediction_window = 12  # This time is at a Weekly level\n",
    "prediction_contained = True\n",
    "retrain_models=True\n",
    "\n",
    "lag_list = [2, 4, 6, 10]\n",
    "rolling_list = [2, 4, 6]\n",
    "cut_date = pd.to_datetime(\"2021-12-26\")\n",
    "evaluation_window = max(lag_list) + max(rolling_list) + 1\n",
    "# ? Note: that we are going to be placed at t='2021-12-26'  the we need to start the iteration at MAX date to cover predictions until '2021-12-26'. Notice also that you will need at least evaluation_window observations before max date in order to create predictions for forecast_start. The code advance one week at a time to recalculate the predictions using previous predictions.\n",
    "\n",
    "if prediction_contained:\n",
    "    if retrain_models:\n",
    "        file_path = \"./data/preprocess/featureengineering_test.csv\"\n",
    "        df_test = pd.read_csv(file_path)\n",
    "        df_test[\"date\"] = pd.to_datetime(df_test[\"date\"])\n",
    "        min_date = df_test.date.min()\n",
    "        prediction_window = 104  # two years\n",
    "        forecast_start = cut_date - timedelta(days=(prediction_window * 7))\n",
    "\n",
    "    else:\n",
    "        prediction_window = 12  # Three months\n",
    "        forecast_start = cut_date - timedelta(days=(prediction_window * 7))\n",
    "        min_date = forecast_start - timedelta(days=(53 * 7))\n",
    "\n",
    "else:\n",
    "    # Start from the last day and starts making iterations over the future\n",
    "    forecast_start = cut_date\n",
    "    min_date = forecast_start - timedelta(days=(53 * 7))\n",
    "\n",
    "for model in [\"lgbm\", \"randomforest\", ]:\n",
    "    start_time = time.time()\n",
    "    if TEST:\n",
    "        file_path = \"./data/preprocess/featureengineering_test.csv\"\n",
    "    else:\n",
    "        file_path = \"./data/preprocess/featureengineering.csv\"\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[~df[\"commit_count\"].isna()]\n",
    "    # Data Preprocessing\n",
    "    df = prepare_data(df, \"date\")  # 30 days\n",
    "    df = pd.read_csv(file_path)\n",
    "    os.chdir(\"./process\")\n",
    "\n",
    "    df_predicted_all = create_iterative_forecast(\n",
    "        df,\n",
    "        model=model,\n",
    "        forecast_start=forecast_start,\n",
    "        lag_list=lag_list,\n",
    "        prediction_window=prediction_window,\n",
    "        rolling_list=rolling_list,\n",
    "        parallel=parallel,\n",
    "        evaluation_window=evaluation_window,\n",
    "        min_date=min_date,\n",
    "        model_features=model_features,\n",
    "    )\n",
    "    # Revert back to the original working directory\n",
    "    os.chdir(\"..\")\n",
    "    end_time = time.time()\n",
    "    elapsed_time = round(end_time - start_time)\n",
    "\n",
    "    logging.info(f\"Time taken for the operation: {elapsed_time} seconds\")\n",
    "    text_file = open(f\"logs_{model}_November.txt\", \"w\")\n",
    "    log_string = f\"\"\"\n",
    "    MODEL: {model}\n",
    "    PROCESSING TIME:  {elapsed_time}\n",
    "\n",
    "    PARALLEL: {parallel}\n",
    "\n",
    "    CORES: 32\n",
    "    SUBPROCESS CORES: 6\n",
    "    \"\"\"\n",
    "    text_file.write(log_string)\n",
    "\n",
    "    # close file\n",
    "    text_file.close()\n",
    "\n",
    "#df_predicted_all is saved with the create_iterative_forecast function at \"../data/process/predictions_{model}.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enamble model\n",
    "Finally all the model predictions are joinned into a single DataFrame that feeds the elasticNet moodel. Additionally, new variables are created to levarage the results of previous models:\n",
    "* **Forecast Horizon**: Indicative variables that tells the elastic net how many weeks in the future is the next wave of prediction is ahead of the last reported week.\n",
    "* **TimeSeries Cluster**:  A cluster algorithm that separates different trends of commit counts series. Each repository is assigned to one cluster out of 8 different clusters. For exmaple, cluster_0 represent all the repositories that reported 0 in most of their commit history.\n",
    "* **Month Control variables**:  These dummies asign 1 or 0 deppending on the month of the weekly reported commits. There are 11 dummis, each one representing a different month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_column = \"date\"\n",
    "prediction_start = pd.to_datetime(\"2022-09-01\")\n",
    "retrain_models = True\n",
    "cut_date = pd.to_datetime(\"2021-12-26\")\n",
    "prediction_contained = True\n",
    "prediction_window = 12\n",
    "lag_list = [2, 4, 6, 10]\n",
    "rolling_list = [2, 4, 6]\n",
    "evaluation_window = max(lag_list) + max(rolling_list) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from forecast_ensemble import  prepare_all_models, create_month_dummy, test_uniqueness_branch_date, create_cluster, cluster_indicator, create_forecast_horizon, train_elasticnet_ensemble_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(current_dir)\n",
    "\n",
    "if prediction_contained:\n",
    "    prediction_start = cut_date - timedelta(days=(prediction_window * 7))\n",
    "    min_date = prediction_start - timedelta(days=(evaluation_window * 7))\n",
    "    prediction_end = cut_date\n",
    "\n",
    "else:\n",
    "    # Start from the last day and starts making iterations over the future\n",
    "    prediction_start = cut_date\n",
    "    min_date = prediction_start - timedelta(days=(evaluation_window * 7))\n",
    "    prediction_end = cut_date + timedelta(\n",
    "        days=(prediction_window * 7)\n",
    "    )  # Prediction of the future\n",
    "\n",
    "if TEST:\n",
    "    all_entries_path = \"./data/preprocess/featureengineering_test.csv\"\n",
    "else:\n",
    "    all_entries_path = \"./data/preprocess/featureengineering.csv\"\n",
    "\n",
    "df_all = pd.read_csv(all_entries_path)\n",
    "\n",
    "os.chdir(current_dir)\n",
    "os.chdir(\"./process\")\n",
    "df_forecast, df_models = prepare_all_models(retrain_models, cut_date=cut_date)\n",
    "df_forecast_month = create_month_dummy(df_models, retrain_models=retrain_models)\n",
    "test_uniqueness_branch_date(df_models)\n",
    "df_forecast_horizon = create_forecast_horizon(df_models.copy())\n",
    "df_target = df_forecast[[\"Repository\", \"Date\", \"Commit Real\"]].copy()\n",
    "df_target = (\n",
    "    df_target.groupby([\"Repository\", \"Date\"])[\"Commit Real\"].first().reset_index()\n",
    ")\n",
    "\n",
    "predicted_labels = create_cluster(\n",
    "    df=df_all, n_cluster=8, retrain_models=retrain_models, target=\"commit_count\"\n",
    ")\n",
    "df_clusters = cluster_indicator(df_all, predicted_labels, index_col = \"repo_name\")\n",
    "\n",
    "df_lr = df_models.merge(df_clusters, on=\"Repository\", how=\"left\")\n",
    "test_uniqueness_branch_date(df_lr)\n",
    "test_uniqueness_branch_date(df_forecast_month)\n",
    "\n",
    "df_model_input = df_lr.merge(\n",
    "    df_forecast_month, on=[\"Repository\", \"Date\"], how=\"inner\"\n",
    ")\n",
    "df_model_input = df_model_input.merge(\n",
    "    df_forecast_horizon, on=[\"Repository\", \"Date\"], how=\"inner\"\n",
    ")\n",
    "df_model_input = df_target.merge(\n",
    "    df_model_input, on=[\"Repository\", \"Date\"], how=\"inner\"\n",
    ")\n",
    "test_uniqueness_branch_date(df_model_input)\n",
    "\n",
    "model, df_results = train_elasticnet_ensemble_model(\n",
    "    df_model_input,\n",
    "    target=\"Commit Real\",\n",
    "    retrain_models=retrain_models,\n",
    "    load_model=True,\n",
    ")\n",
    "\n",
    "df_results[\"model_family\"] = \"elasticnet\"\n",
    "date_prediction_cut = pd.to_datetime(\"2023-08-01\")\n",
    "df_forecast = pd.concat(\n",
    "    [df_forecast, df_results], ignore_index=True\n",
    ")  # Check columns\n",
    "os.chdir(\"..\")\n",
    "\n",
    "path_out = f\"../data/process/final_predictions.csv\"\n",
    "df_forecast.to_csv(\"final_predictions.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
