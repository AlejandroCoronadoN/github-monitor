{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecast pipeline\n",
    "In this script I will guide you to the execution of the ML forecast pipeline. I will add comments about the pipeline process, model trainning and mthe predictive process. Each stage of the proces can be run independently but we can review and compare our results for a more interactiv experience.\n",
    "\n",
    "## Preprocess\n",
    "The Preprocess scritpt has it's own jupyter notebook where I made an exploratory analysis of the information. In the script verison I only compile the preprocess functions that allows us to run the ML pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta, datetime\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the 'preprocess' directory to the Python path\n",
    "current_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "preprocess_dir = os.path.abspath(os.path.join(current_dir, \"preprocess\"))\n",
    "sys.path.append(preprocess_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the load_raw_data function directly\n",
    "from preprocess_script import load_raw_data, calculate_sample_size, preprocess_data\n",
    "from complete_series import create_weekly_date_dataframe, expand_time_series, create_seasonal_controls, impute_default\n",
    "from feature_engineering import discard_uncompleted_windows, moving_average_variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Complete Series env variables\n",
    "TEST = True\n",
    "date_column = \"date\"\n",
    "aggregation_cols = [\"year\", \"month\", \"week\"]\n",
    "# Feature Engineering env variables\n",
    "date_column = \"date\"\n",
    "lag_list = [2, 4, 6, 10]\n",
    "rolling_list = [2, 4, 6]\n",
    "evaluation_window = max(lag_list) + max(rolling_list) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your raw data file\n",
    "raw_data_path = os.path.join(current_dir, \"data\", \"raw\", \"commit_history_raw.csv\")\n",
    "\n",
    "# Now you can use load_raw_data in your main script\n",
    "df_raw = load_raw_data(raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the sample size\n",
    "total_repositories = len(df_raw.groupby([\"repo_author_single\", \"year\", \"week_number\"])[\"commit_count\"].sum())\n",
    "sample_size = calculate_sample_size(total_repositories)\n",
    "\n",
    "# Preprocess the data\n",
    "preprocess_data(df_raw, sample_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST:\n",
    "    df = pd.read_csv(\"./data/preprocess/commit_history_subset_test.csv\")\n",
    "else:\n",
    "    df = pd.read_csv(\"./data/preprocess/commit_history_subset.csv\")\n",
    "\n",
    "group_id = [\"repo_name\"]\n",
    "\n",
    "df_all = pd.DataFrame()\n",
    "df[date_column] = pd.to_datetime(df[date_column])\n",
    "df_index = df.groupby(group_id).first().reset_index()\n",
    "\n",
    "start_date = df[date_column].min()\n",
    "end_date = df[date_column].max()\n",
    "\n",
    "df_dates_week = create_weekly_date_dataframe(\n",
    "    start_date, end_date, week_start=\"sunday\"\n",
    ")  # Choose between sunday or monday\n",
    "df_expand = expand_time_series(df, date_column, df_index, df_dates_week)\n",
    "df_expand = create_seasonal_controls(df_expand, date_column=\"date\")\n",
    "df_all_preproc = impute_default(df_expand, [\"commit_count\"], 0)\n",
    "df_all_preproc = df_all_preproc[\n",
    "    [\"repo_name\", \"year\", \"commit_count\", \"date\", \"month\", \"week\"]\n",
    "]\n",
    "if TEST:\n",
    "    df_all_preproc.to_csv(\n",
    "        \"./data/preprocess/commit_series_expansion_test.csv\",\n",
    "        index=False,\n",
    "    )\n",
    "else:\n",
    "    df_all_preproc.to_csv(\n",
    "        \"./data/preprocess/commit_series_expansion.csv\",\n",
    "        index=False,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Enginnering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TEST:\n",
    "    df = pd.read_csv(\"./data/preprocess/commit_series_expansion_test.csv\")\n",
    "else:\n",
    "    df = pd.read_csv(\"./data/preprocess/commit_series_expansion.csv\")\n",
    "\n",
    "df_window_mean, df_window_ewm =moving_average_variables(df, date_column, lag_list, rolling_list)\n",
    "\n",
    "df_out = df.merge(df_window_mean, on=[date_column, \"repo_name\"], how=\"inner\")\n",
    "df_out = df_out.merge(df_window_ewm, on=[date_column, \"repo_name\"], how=\"inner\")\n",
    "df_out = df_out.sort_values([\"repo_name\", date_column], ascending=True)\n",
    "\n",
    "df_out[date_column] = pd.to_datetime(df_out[date_column])\n",
    "\n",
    "if TEST:\n",
    "    file_path = \"./data/preprocess/featureengineering_test.csv\"\n",
    "else:\n",
    "    file_path = \"./data/preprocess/featureengineering.csv\"\n",
    "df_out = discard_uncompleted_windows(df_out, evaluation_window, date_column, \"W\")\n",
    "df_out.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "# Hyperparameter Optimization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append the 'preprocess' directory to the Python path\n",
    "current_dir = os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "preprocess_dir = os.path.abspath(os.path.join(current_dir, \"process\"))\n",
    "sys.path.append(preprocess_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperparameter_optimization import preprocess_data, hyperparameter_optimization\n",
    "from iterative_prediction import create_iterative_forecast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the previous script\n",
    "lag_list = [2, 4, 6, 10]\n",
    "rolling_list = [2, 4, 6]\n",
    "date_column = \"date\"\n",
    "evaluation_window = (\n",
    "    max(lag_list) * 7 + max(rolling_list) * 7\n",
    ")  # minimum data to run t\n",
    "prediction_window = 7 * 12  # days_in_week*number_weeks\n",
    "cut_date = pd.to_datetime(\"2021-12-26\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if TEST:\n",
    "    file_path = \"./data/preprocess/featureengineering_test.csv\"\n",
    "else:\n",
    "    file_path = \"./data/preprocess/featureengineering.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df = df[~df[\"commit_count\"].isna()]\n",
    "\n",
    "# Data Preprocessing\n",
    "df = preprocess_data(df, date_column)  # 30 days\n",
    "hyperparameter_optimization(\n",
    "    df= df, \n",
    "    target =\"commit_count\", \n",
    "    prediction_window = prediction_window, \n",
    "    evaluation_window = evaluation_window,\n",
    "    cut_date=cut_date)\n",
    "\n",
    "#! NOTE: No data is saved, we only tarinned the models and save them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST = True\n",
    "parallel = False\n",
    "prediction_window = 12  # This time is at a Weekly level\n",
    "prediction_contained = True\n",
    "\n",
    "lag_list = [2, 4, 6, 10]\n",
    "rolling_list = [2, 4, 6]\n",
    "cut_date = pd.to_datetime(\"2021-12-26\")\n",
    "evaluation_window = max(lag_list) + max(rolling_list) + 1\n",
    "# ? Note: that we are going to be placed at t='2021-12-26'  the we need to start the iteration at MAX date to cover predictions until '2021-12-26'. Notice also that you will need at least evaluation_window observations before max date in order to create predictions for forecast_start. The code advance one week at a time to recalculate the predictions using previous predictions.\n",
    "\n",
    "if prediction_contained:\n",
    "    forecast_start = cut_date - timedelta(\n",
    "        days=(prediction_window * 7)\n",
    "    )\n",
    "    min_date = forecast_start - timedelta(days=(evaluation_window * 7))\n",
    "else:\n",
    "    #Start from the last day and starts making iterations over the future\n",
    "    forecast_start = cut_date\n",
    "    min_date = forecast_start - timedelta(days=(evaluation_window * 7))\n",
    "\n",
    "for model in [\"xgboost\", \"randomforest\"]:\n",
    "    start_time = time.time()\n",
    "    if TEST:\n",
    "        file_path = \"./data/preprocess/featureengineering_test.csv\"\n",
    "    else:\n",
    "        file_path = \"./data/preprocess/featureengineering.csv\"\n",
    "\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df[~df[\"commit_count\"].isna()]\n",
    "    # Data Preprocessing\n",
    "    df = preprocess_data(df, \"date\")  # 30 days\n",
    "    df = pd.read_csv(file_path)\n",
    "    df_predicted_all = create_forcast(df, forecast_start= forecast_start, parallel=parallel, evaluation_window=evaluation_window, min_date=min_date)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = round(end_time - start_time)\n",
    "\n",
    "    logging.info(f\"Time taken for the operation: {elapsed_time} seconds\")\n",
    "    # SAVE\n",
    "    df_predicted_all.to_csv(f\"./data/process/predictions_{model}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
