{"cells":[{"cell_type":"markdown","metadata":{"_cell_guid":"2b05d699-08e2-4e83-9080-a5f354bd97dc","_uuid":"c558f135d07a4c5ab4f8db3361a6a368f55e1c38"},"source":["## BigQuery -  Github\n","This basic Python kernel shows you how to query the `commits` table in the GitHub Repos BigQuery dataset. We will use this information to obatin a representative sample of all the public repositories at Github. To run "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["from google.cloud import bigquery\n","import pandas as pd\n","import math\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"99307486-b708-437a-8513-a0a31fda1538","_uuid":"445fa22e24fb4790f811d7295f986c633832e864","scrolled":true,"trusted":true},"outputs":[],"source":["\n","client = bigquery.Client()\n","QUERY = \"\"\"\n","        SELECT *\n","        FROM `bigquery-public-data.github_repos.commits`\n","        LIMIT 2000\n","        \"\"\"\n","\n","query_job = client.query(QUERY)\n","\n","iterator = query_job.result(timeout=30)\n","rows = list(iterator)\n","\n","commit_messages = pd.DataFrame(data=[list(x.values()) for x in rows], columns=list(rows[0].keys()))\n","\n","# Look at the first 10 headlines\n","commit_messages.head(10)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","def get_unique_repo_names(dataset_id='bigquery-public-data', table_id='github_repos.commits', limit=2000):\n","    \"\"\"\n","    Get all unique values of the 'repo_name' variable in a BigQuery table.\n","\n","    Parameters:\n","    - dataset_id (str): The ID of the dataset containing the table.\n","    - table_id (str): The ID of the table.\n","    - limit (int): The maximum number of rows to retrieve.\n","\n","    Returns:\n","    - pd.Series: A Pandas Series containing unique repo names.\n","    \"\"\"\n","    client = bigquery.Client()\n","\n","    # Build the SQL query\n","    if limit:\n","        raise ValueError()\n","        query = f\"\"\"\n","            SELECT DISTINCT repo_name\n","            FROM `{dataset_id}.{table_id}`, UNNEST(repo_name) AS repo_name\n","            LIMIT {limit}\n","        \"\"\"\n","    else: \n","        query = f\"\"\"\n","            SELECT DISTINCT repo_name\n","            FROM `{dataset_id}.{table_id}`, UNNEST(repo_name) AS repo_name\n","        \"\"\"\n","    \n","    # Run the query\n","    query_job = client.query(query)\n","    iterator = query_job.result(timeout=30)\n","    rows = list(iterator)\n","\n","    # Create a Pandas Series from the results\n","    repo_names = pd.Series([row['repo_name'] for row in rows], name='repo_name')\n","\n","    return repo_names\n","\n","unique_repo_names = get_unique_repo_names(limit=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Subsample \n","Now that we have the total amount of repositories in the Population, we now can create a rule of thumb to obtain a representative sample of the population. I'm obtaining a subsample of data to reduce the training time of the models and still preserve the original distribution of the population. In a more constraint scenario I would evaluate that the sample also preserve the proportion of all the different languages that appear in the population distributio, however, for this challeange I will simplify the problem and only obtain 9,900 random repositories. For a production version of this application we could use the entire population in the Database and train the model in an multicore Ec2 instance. Also, for a more robust smaple extraction,we can implement an startified sampling of the repositories.\n","\n","\n","Stratified sampling is a sampling technique involving the division of the total population into smaller subgroups, known as strata, and subsequently taking a sample from each stratum in proportion to its size in the overall population. This methodology aims to ensure that all subgroups are adequately represented in the final sample. In the context of analyzing GitHub repositories, applying a stratified approach would involve identifying different categories of repositories, such as those written in different programming languages or belonging to various application domains. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","def calculate_sample_size(population_size, confidence_level=0.95, margin_of_error=0.01):\n","    z_score = 1.96  # for a 95% confidence level\n","    p = 0.5  # assuming a conservative estimate for proportion\n","    numerator = z_score ** 2 * p * (1 - p)\n","    denominator = margin_of_error ** 2\n","    sample_size = math.ceil((numerator / denominator) / (1 + ((numerator - 1) / population_size)))\n","    return sample_size\n","\n","total_repositories = 3_300_000\n","sample_size = calculate_sample_size(total_repositories)\n","print(f\"Recommended sample size: {sample_size}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","def get_representative_sample(repo_names, sample_size=10, seed=42):\n","    \"\"\"\n","    Get a representative sample of repositories from the provided list.\n","\n","    Parameters:\n","    - repo_names (pd.Series): Pandas Series containing repository names.\n","    - sample_size (int): The size of the representative sample.\n","    - seed (int): Seed for the random number generator.\n","\n","    Returns:\n","    - list: A list containing a representative sample of repository names.\n","    \"\"\"\n","    # Set the seed for reproducibility\n","    random.seed(seed)\n","\n","    # Check if the sample size is greater than the total number of repositories\n","    if sample_size > len(repo_names):\n","        raise ValueError(\"Sample size cannot be greater than the total number of repositories.\")\n","\n","    # Get a representative sample using random sampling\n","    sample = random.sample(repo_names.tolist(), sample_size)\n","\n","    return sample\n","\n","# Example usage\n","# Assuming you already have a Pandas Series named unique_repo_names\n","subset_sample = get_representative_sample(unique_repo_names, sample_size=sample_size, seed=123)\n","\n","# Print the representative sample\n","print(len(subset_sample))\n","subset_sample[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["query = \"\"\"\n","    SELECT\n","        repo_name, \n","        COUNT(*) AS commit_count,\n","        year, week_number\n","    FROM\n","\n","    (SELECT\n","        ARRAY_TO_STRING(repo_name, ',') AS repo_name,\n","        FORMAT_TIMESTAMP('%Y%m%d', TIMESTAMP_SECONDS(committer.date.seconds)) AS date,\n","        EXTRACT(YEAR FROM TIMESTAMP_SECONDS(committer.date.seconds)) AS year,\n","        EXTRACT(ISOWEEK FROM TIMESTAMP_SECONDS(committer.date.seconds)) AS week_number,\n","        EXTRACT(MONTH FROM TIMESTAMP_SECONDS(committer.date.seconds)) AS month,\n","        EXTRACT(DAY FROM TIMESTAMP_SECONDS(committer.date.seconds)) AS day,\n","    FROM\n","        `bigquery-public-data.github_repos.commits`) A\n","        \n","    GROUP BY\n","        A.repo_name, A.year, A.week_number\n","\"\"\"\n","\n","\n","\n","\n","query_job = client.query(query)\n","iterator = query_job.result()\n","rows = list(iterator)\n","result_df = pd.DataFrame(data=[list(x.values()) for x in rows], columns=list(rows[0].keys()))\n"]},{"cell_type":"markdown","metadata":{},"source":["## Preprocess Data\n","Now that we have all the information for a subsample of 9900 repositories, I want to extract the required variables for our Forecast models. The main variable for this exercise will be the date and the number of commits per week. This information is contained inside the commiter variables (represented as a json). After appliying preprocess_data function we are ready to start exploring our data and create assumptions for the forecasting models."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["result_df.to_csv(\"../../data/commit_history.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sample_df = result_df[result_df.repo_name.isin(subset_sample)]\n","result_df.to_csv(\"../../data/commit_history_sample.csv\")"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":6178,"sourceId":337545,"sourceType":"datasetVersion"}],"dockerImageVersionId":44,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":4}
